{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSCI 4253 / 5253 - Lab #4 - Patent Problem with Spark RDD - SOLUTION\n",
    "<div>\n",
    " <h2> CSCI 4283 / 5253 \n",
    "  <IMG SRC=\"https://www.colorado.edu/cs/profiles/express/themes/cuspirit/logo.png\" WIDTH=50 ALIGN=\"right\"/> </h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This [Spark cheatsheet](https://s3.amazonaws.com/assets.datacamp.com/blog_assets/PySpark_SQL_Cheat_Sheet_Python.pdf) is useful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "import numpy as np\n",
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf=SparkConf().setAppName(\"Lab4-rdd\").setMaster(\"local[*]\")\n",
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using PySpark and RDD's on the https://coding.csel.io machines is slow -- most of the code is executed in Python and this is much less efficient than the java-based code using the PySpark dataframes. Be patient and trying using `.cache()` to cache the output of joins. You may want to start with a reduced set of data before running the full task. You can use the `sample()` method to extract just a sample of the data or use "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These two RDD's are called \"rawCitations\" and \"rawPatents\" because you probably want to process them futher (e.g. convert them to integer types, etc). \n",
    "\n",
    "The `textFile` function returns data in strings. This should work fine for this lab.\n",
    "\n",
    "Other methods you use might return data in type `Byte`. If you haven't used Python `Byte` types before, google it. You can convert a value of `x` type byte into e.g. a UTF8 string using `x.decode('uft-8')`. Alternatively, you can use the `open` method of the gzip library to read in all the lines as UTF-8 strings like this:\n",
    "```\n",
    "import gzip\n",
    "with gzip.open('cite75_99.txt.gz', 'rt',encoding='utf-8') as f:\n",
    "    rddCitations = sc.parallelize( f.readlines() )\n",
    "```\n",
    "This is less efficient than using `textFile` because `textFile` would use the underlying HDFS or other file system to read the file across all the worker nodes while the using `gzip.open()...readlines()` will read all the data in the frontend and then distribute it to all the worker nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "rddCitations = sc.textFile(\"cite75_99.txt.gz\")\n",
    "rddPatents = sc.textFile(\"apat63_99.txt.gz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data looks like the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\"CITING\",\"CITED\"',\n",
       " '3858241,956203',\n",
       " '3858241,1324234',\n",
       " '3858241,3398406',\n",
       " '3858241,3557384']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rddCitations.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\"PATENT\",\"GYEAR\",\"GDATE\",\"APPYEAR\",\"COUNTRY\",\"POSTATE\",\"ASSIGNEE\",\"ASSCODE\",\"CLAIMS\",\"NCLASS\",\"CAT\",\"SUBCAT\",\"CMADE\",\"CRECEIVE\",\"RATIOCIT\",\"GENERAL\",\"ORIGINAL\",\"FWDAPLAG\",\"BCKGTLAG\",\"SELFCTUB\",\"SELFCTLB\",\"SECDUPBD\",\"SECDLWBD\"',\n",
       " '3070801,1963,1096,,\"BE\",\"\",,1,,269,6,69,,1,,0,,,,,,,',\n",
       " '3070802,1963,1096,,\"US\",\"TX\",,1,,2,6,63,,0,,,,,,,,,',\n",
       " '3070803,1963,1096,,\"US\",\"IL\",,1,,2,6,63,,9,,0.3704,,,,,,,',\n",
       " '3070804,1963,1096,,\"US\",\"OH\",,1,,2,6,63,,3,,0.6667,,,,,,,']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rddPatents.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In other words, they are a single string with multiple CSV's. You will need to convert these to (K,V) pairs, probably convert the keys to `int` and so on. You'll need to `filter` out the header string as well since there's no easy way to extract all the lines except the first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 1: RDD dev mode (as suggested in the README) ---\n",
    "# RDD joins are slow, so I develop/debug on a 5% sample first.\n",
    "# Same exact logic applies to the full dataset if needed, simply remove .sample(False, 0.05, seed=42)\n",
    "\n",
    "rddPatents_dev = rddPatents\n",
    "rddCitations_dev = rddCitations\n",
    "\n",
    "import csv\n",
    "\n",
    "# Debugging fix: header rows can slip through in RDD text files.\n",
    "# The safest filter is: keep only rows that begin with a digit.\n",
    "def is_data_row(line):\n",
    "    return len(line) > 0 and line[0].isdigit()\n",
    "\n",
    "def parse_patent(line):\n",
    "    row = next(csv.reader([line]))\n",
    "    patent = int(row[0])\n",
    "    state = row[5] if row[5] != \"\" else None\n",
    "    return (patent, state)\n",
    "\n",
    "def parse_citation(line):\n",
    "    parts = line.split(\",\")\n",
    "    citing = int(parts[0])\n",
    "    cited = int(parts[1])\n",
    "    return (cited, citing)  # key=CITED for the first join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Set up a development sample + parsing helpers\n",
    "\n",
    "For the RDD solution, I used a 5% sample of the patents and citations data while developing and debugging, as suggested in the README. RDD joins are much slower than DataFrame joins, so working on a smaller subset helps confirm the logic without waiting a long time for each join step.\n",
    "\n",
    "In this chunk, I also define small helper functions for parsing. Because the input files are raw text with a header row, I filter out non-data rows by keeping only lines that begin with a digit. This prevents type conversion errors when casting the patent ID and citation IDs to integers.\n",
    "\n",
    "**Debugging note:**  \n",
    "While parsing the patent RDD, an initial header filter based on string matching allowed a header row to pass through, causing a type conversion error when casting the patent ID to an integer. This was resolved by filtering input rows to only those that begin with a digit, which reliably distinguishes data rows from headers in the raw text file. This approach is more robust for RDD-based parsing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(3858264, ((3203002, None), None)),\n",
       " (4922568, ((3203002, None), None)),\n",
       " (3858291, ((3768550, None), None)),\n",
       " (3858324, ((2423786, None), None)),\n",
       " (3858348, ((1384972, None), None)),\n",
       " (3858402, ((3720066, None), None)),\n",
       " (3858429, ((3051216, None), None)),\n",
       " (3858444, ((3710874, None), None)),\n",
       " (3858444, ((3613853, None), None)),\n",
       " (3858480, ((3435722, None), None))]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Step 2: Build patent->state mapping + attach states to citations ---\n",
    "# Goal: produce an intermediate RDD where each citation has both states available\n",
    "# (CITING, (CITED, CITED_STATE, CITING_STATE)).\n",
    "\n",
    "pat_states = (\n",
    "    rddPatents_dev\n",
    "    .filter(is_data_row)\n",
    "    .map(parse_patent)\n",
    "    .filter(lambda x: x[1] is not None)     # keep only patents with a real state\n",
    "    .cache()\n",
    ")\n",
    "\n",
    "cit_by_cited = (\n",
    "    rddCitations_dev\n",
    "    .filter(is_data_row)\n",
    "    .map(parse_citation)                   # (CITED, CITING)\n",
    "    .cache()\n",
    ")\n",
    "\n",
    "# Join 1: attach CITED_STATE\n",
    "cited_join = (\n",
    "    cit_by_cited\n",
    "    .leftOuterJoin(pat_states)             # (CITED, (CITING, CITED_STATE))\n",
    "    .cache()\n",
    ")\n",
    "cited_join.count()  # force caching so we don't recompute the join later\n",
    "\n",
    "# Re-key by CITING so we can attach CITING_STATE\n",
    "by_citing = (\n",
    "    cited_join\n",
    "    .map(lambda x: (x[1][0], (x[0], x[1][1])))  # (CITING, (CITED, CITED_STATE))\n",
    "    .cache()\n",
    ")\n",
    "\n",
    "# Join 2: attach CITING_STATE\n",
    "both_join = (\n",
    "    by_citing\n",
    "    .leftOuterJoin(pat_states)                 # (CITING, ((CITED, CITED_STATE), CITING_STATE))\n",
    "    .cache()\n",
    ")\n",
    "both_join.count()  # force caching again (RDD joins are slow)\n",
    "\n",
    "both_join.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Build patent → state mapping and attach states onto citations (two joins)\n",
    "\n",
    "In this chunk, I construct the intermediate dataset needed to count same-state citations. First, I extract a clean mapping of `(PATENT, POSTATE)` from the patents file, keeping only rows that have valid state information. This ensures later state comparisons are meaningful.\n",
    "\n",
    "Next, I join state information onto the citations data in two steps. I key citations by `CITED` to attach the **cited patent’s state** (`CITED_STATE`) using a left outer join, since some cited patents may not exist in the patents table. Then I re-key by `CITING` and join again to attach the **citing patent’s state** (`CITING_STATE`). I cache and materialize the join outputs using `count()` so Spark does not recompute these expensive joins later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PATENT\tSAME_STATE\n",
      "5069283\t2\n",
      "5544893\t2\n",
      "4578858\t1\n",
      "4000494\t1\n",
      "5601846\t1\n",
      "4257009\t1\n",
      "4431513\t1\n",
      "4434258\t1\n",
      "4439634\t1\n",
      "4456845\t1\n",
      "(5069283, ['1991', '11659', '1989', 'US', 'OK', '624265', '2', '26', '166', '6', '64', '38', '7', '1', '0.4082', '0.6274', '4.4286', '10.5526', '0.0286', '0.0263', '0.1429', '0.1429', 2])\n",
      "(4578858, ['1986', '9587', '1984', 'US', 'TX', '601765', '2', '22', '29', '5', '52', '15', '1', '1', '0', '0.6933', '7', '8', '0.9091', '0.6667', '0', '0', 1])\n",
      "(4000494, ['1976', '6206', '1975', 'US', 'PA', '256580', '2', '9', '346', '4', '49', '8', '3', '0.75', '0', '0', '3.3333', '9.25', '0.1667', '0.125', '1', '1', 1])\n",
      "(5601846, ['1997', '13556', '1995', 'US', 'NY', '709931', '2', '22', '424', '3', '31', '110', '21', '0.9364', '0.4943', '0.8031', '1.381', '14.1636', '0', '0', '0.9524', '0.9524', 1])\n",
      "(4257009, ['1981', '7746', '1979', 'US', 'NJ', '466175', '2', '2', '330', '4', '41', '5', '9', '1', '0.6914', '0.8', '13.4444', '12.2', '1', '0.6', '0', '0', 1])\n",
      "(4431513, ['1984', '8810', '1982', 'US', 'OH', '595200', '2', '16', '208', '1', '19', '8', '7', '0.625', '0.5714', '0.64', '8.7143', '21.375', '0.5', '0.125', '0', '0', 1])\n",
      "(4434258, ['1984', '8824', '1982', 'US', 'DE', '165265', '2', '18', '524', '1', '15', '8', '23', '1', '0.7025', '0.2188', '7.4091', '5.625', '0.375', '0.375', '0.2381', '0.2174', 1])\n",
      "(4439634, ['1984', '8852', '1982', 'US', 'CA', '234400', '2', '28', '455', '2', '21', '7', '0', '1', '', '0.6531', '', '7.5714', '0', '0', '', '', 1])\n",
      "(4456845, ['1984', '8943', '1983', 'US', 'IN', '218550', '2', '44', '310', '4', '45', '8', '11', '1', '0.6281', '0.2188', '7.3636', '13', '0.8', '0.5', '0.8182', '0.8182', 1])\n",
      "(5544893, ['1996', '13374', '1995', 'US', 'NV', '722779', '2', '16', '273', '6', '62', '63', '8', '0.8889', '0.2188', '0.5319', '1.875', '15.4127', '0', '0', '0', '0', 2])\n"
     ]
    }
   ],
   "source": [
    "# --- Step 3: Count same-state citations + report results ---\n",
    "# SAME_STATE = count of citations where cited and citing states are both present and match.\n",
    "\n",
    "same_state_counts = (\n",
    "    both_join\n",
    "    .filter(lambda x: x[1][0][1] is not None and x[1][1] is not None)  # has CITED_STATE and CITING_STATE\n",
    "    .filter(lambda x: x[1][0][1] == x[1][1])                           # states match\n",
    "    .map(lambda x: (x[0], 1))                                          # (CITING, 1)\n",
    "    .reduceByKey(lambda a, b: a + b)                                   # (CITING, SAME_STATE)\n",
    "    .cache()\n",
    ")\n",
    "\n",
    "top10 = same_state_counts.takeOrdered(10, key=lambda x: -x[1])\n",
    "\n",
    "print(\"PATENT\\tSAME_STATE\")\n",
    "for pid, cnt in top10:\n",
    "    print(f\"{pid}\\t{cnt}\")\n",
    "\n",
    "# --- Optional: match the “(PATENT, [row..., SAME_STATE])” reporting style ---\n",
    "# This is just for display so the output looks like the reference/example.\n",
    "\n",
    "def parse_full_patent(line):\n",
    "    row = next(csv.reader([line]))\n",
    "    patent = int(row[0])\n",
    "    return (patent, row[1:])  # everything except PATENT\n",
    "\n",
    "patents_full = (\n",
    "    rddPatents_dev\n",
    "    .filter(is_data_row)\n",
    "    .map(parse_full_patent)\n",
    "    .cache()\n",
    ")\n",
    "\n",
    "top10_rdd = sc.parallelize(top10)  # (PATENT, SAME_STATE)\n",
    "\n",
    "final_join = (\n",
    "    top10_rdd\n",
    "    .join(patents_full)                          # (PATENT, (SAME_STATE, full_row))\n",
    "    .map(lambda x: (x[0], x[1][1] + [x[1][0]]))   # append SAME_STATE at end\n",
    ")\n",
    "\n",
    "for row in final_join.take(10):\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 34: Count SAME_STATE citations and report top results\n",
    "\n",
    "In this chunk, I compute the final value required by the assignment: the number of same-state citations per citing patent. A same-state citation is defined as a citation where both the citing and cited patents have state information and the states match.\n",
    "\n",
    "Using the joined RDD from the previous chunk, I filter out records with missing states, filter again to keep only matching-state pairs, and then map each remaining row to `(CITING, 1)`. I use `reduceByKey` to sum these values and produce `(CITING, SAME_STATE)` counts.\n",
    "\n",
    "Finally, I report the top 10 patents by SAME_STATE. For display purposes, I also join the top-10 results back to the full patent rows so the output can be printed in the same “(PATENT, [row…, SAME_STATE])” style as the example/reference output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
